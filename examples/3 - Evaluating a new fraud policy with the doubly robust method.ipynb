{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f8aeeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ope.methods import doubly_robust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23784f7e",
   "metadata": {},
   "source": [
    "### Example using the doubly robust (DR) method to offline evaluate a new fraud policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7852be47",
   "metadata": {},
   "source": [
    "#### 1 - Assume we have a fraud model in production that blocks transactions if the P(fraud) > 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac18fdba",
   "metadata": {},
   "source": [
    "Let's build some sample logs from that policy running in production. One thing to note, we need some basic exploration in the production logs (e.g. epsilon-greedy w/Îµ = 0.1). That is, 10% of the time we take a random action. Rewards represent revenue gained from allowing the transaction. A negative reward indicates the transaction was fraud and resulted in a chargeback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cd1717e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>action</th>\n",
       "      <th>action_prob</th>\n",
       "      <th>reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'p_fraud': 0.08}</td>\n",
       "      <td>blocked</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'p_fraud': 0.03}</td>\n",
       "      <td>allowed</td>\n",
       "      <td>0.9</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'p_fraud': 0.02}</td>\n",
       "      <td>allowed</td>\n",
       "      <td>0.9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'p_fraud': 0.01}</td>\n",
       "      <td>allowed</td>\n",
       "      <td>0.9</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'p_fraud': 0.09}</td>\n",
       "      <td>allowed</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'p_fraud': 0.4}</td>\n",
       "      <td>allowed</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             context   action  action_prob  reward\n",
       "0  {'p_fraud': 0.08}  blocked          0.9       0\n",
       "1  {'p_fraud': 0.03}  allowed          0.9      20\n",
       "2  {'p_fraud': 0.02}  allowed          0.9      10\n",
       "3  {'p_fraud': 0.01}  allowed          0.9      20\n",
       "4  {'p_fraud': 0.09}  allowed          0.1     -20\n",
       "5   {'p_fraud': 0.4}  allowed          0.1     -10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs_df = pd.DataFrame([\n",
    "    {\"context\": {\"p_fraud\": 0.08}, \"action\": \"blocked\", \"action_prob\": 0.90, \"reward\": 0},\n",
    "    {\"context\": {\"p_fraud\": 0.03}, \"action\": \"allowed\", \"action_prob\": 0.90, \"reward\": 20},\n",
    "    {\"context\": {\"p_fraud\": 0.02}, \"action\": \"allowed\", \"action_prob\": 0.90, \"reward\": 10}, \n",
    "    {\"context\": {\"p_fraud\": 0.01}, \"action\": \"allowed\", \"action_prob\": 0.90, \"reward\": 20},     \n",
    "    {\"context\": {\"p_fraud\": 0.09}, \"action\": \"allowed\", \"action_prob\": 0.10, \"reward\": -20}, # only allowed due to exploration \n",
    "    {\"context\": {\"p_fraud\": 0.40}, \"action\": \"allowed\", \"action_prob\": 0.10, \"reward\": -10}, # only allowed due to exploration     \n",
    "])\n",
    "\n",
    "logs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ddd5bb",
   "metadata": {},
   "source": [
    "#### 2 - Now let's use the doubly robust method to score a more lenient fraud model that blocks transactions only if the P(fraud) > 0.10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062ca5df",
   "metadata": {},
   "source": [
    "The doubly robust method requires that we have a function that computes `P(action | context)`for all possible actions under our new policy. We can define that for our new policy easily here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68a0b906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_probabilities(context):\n",
    "    epsilon = 0.10\n",
    "    if context[\"p_fraud\"] > 0.10:\n",
    "        return {\"allowed\": epsilon, \"blocked\": 1 - epsilon}    \n",
    "    \n",
    "    return {\"allowed\": 1 - epsilon, \"blocked\": epsilon}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f2963c",
   "metadata": {},
   "source": [
    "We will use the same production logs above and run them through the new policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf7e2292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'expected_reward_logging_policy': {'mean': 2.53,\n",
       "  'ci_low': -8.42,\n",
       "  'ci_high': 13.48},\n",
       " 'expected_reward_new_policy': {'mean': -18.22,\n",
       "  'ci_low': -97.56,\n",
       "  'ci_high': 61.12}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doubly_robust.evaluate(logs_df, action_probabilities, num_bootstrap_samples=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062ac2e2",
   "metadata": {},
   "source": [
    "The doubly robust method estimates that the expected reward per observation for the new policy is much worse than the logging policy so we wouldn't roll out this new policy into an A/B test or production and instead should test some different policies offline.\n",
    "\n",
    "However, the confidence intervals around the expected rewards for our old and new policies overlap heavily. If we want to be really certain, it's might be best to gather some more data to ensure the difference is signal and not noise. In this case, fortunately, we have strong reason to suspect the new policy is worse, but these confidence intervals can be important in cases where we have less prior certainty."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
